[{"id":0,"title":"About byorg.ai","content":"#\n\n\nIntroduction#\n\nbyorg.ai is a framework designed for rapid development and deployment of AI\nassistants within companies and organizations.\n\n\nSupported Integrations#\n\n * Slack\n * Discord\n\nbyorg.ai supports a wide range of large language models (LLMs) via the Vercel AI\nSDK. You can host byorg.ai applications on various cloud platforms or local\nenvironments. We provide examples for some popular hosting options.","routePath":"/docs/about","lang":"","toc":[{"text":"Introduction","id":"introduction","depth":2,"charIndex":3},{"text":"Supported Integrations","id":"supported-integrations","depth":2,"charIndex":143}],"domain":"","frontmatter":{},"version":""},{"id":1,"title":"Chat Model","content":"#\n\n\nProviders and Adapter#\n\nYou can use any AI provider supported by Vercel’s AI SDK. This includes both\nLLM-as-a-service providers like OpenAI, Anthropic, and others, as well as\nlocally hosted LLMs. We are also open to extending support to other types of\nchat models, such as LangChain’s runnables.\n\n\nProviders Examples#\n\n\n\nAfter instantiating the provider client, wrap it with our VercelAdapter class:\n\n\n\nNow that the chatModel is ready, let’s discuss the systemPrompt function.","routePath":"/docs/core/chat-model","lang":"","toc":[{"text":"Providers and Adapter","id":"providers-and-adapter","depth":2,"charIndex":3},{"text":"Providers Examples","id":"providers-examples","depth":3,"charIndex":301}],"domain":"","frontmatter":{},"version":""},{"id":2,"title":"Context","content":"#\n\nThe context object holds information about the currently processed message. It\nallows you to modify the behavior of your assistant at runtime or alter the\nmessage processing flow.\n\nContext can be modified by middlewares during the message processing flow to\nimplement highly flexible logic or rules (e.g., authentication, RAG, etc.).\n\n\nProperties in Context#\n\n\n\nTo add typing for your custom properties to the context, create a file with the\ntype definition and override the typing.\n\n\n\nWARNING\n\nAll custom properties must be optional, as the current context creation does not\nsupport default values for custom objects.\n\nAfter setting extras, you can access them from the context object:\n\n\n\nNext, we’ll explore the concept of plugins to understand how to modify the\ncontext.","routePath":"/docs/core/context","lang":"","toc":[{"text":"Properties in Context","id":"properties-in-context","depth":3,"charIndex":338}],"domain":"","frontmatter":{},"version":""},{"id":3,"title":"Error handling","content":"#\n\nThe error handler in byorg.ai is responsible for processing error objects and\nreturning messages that are sent back to the user. You can customize the error\nhandling by providing your own error handler function. This allows you to define\nspecific reactions to errors and deliver appropriate feedback to users.\n\n\n\nBy implementing a custom error handler, you can tailor the user experience by\nproviding meaningful responses to errors encountered within the byorg framework.","routePath":"/docs/core/error-handling","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":4,"title":"Performance","content":"#\n\nTo test your application's performance, you can use the performance object\navailable in the context.\n\n\n\nAfter collecting your performance data, you can access it through the same\nperformance object. Performance tracking requires all processes to complete, so\nit uses effect instead of middleware, as it runs after the response is\nfinalized.\n\n\n\n\nMeasures vs Marks#\n\nThis concept is inspired by the Web Performance API. Marks are essentially named\nsequences that the performance tool uses to measure execution time. For\ninstance, if you have a tool for your AI and want to evaluate its performance,\nyou might find it triggered multiple times by the AI. Therefore, a single mark\ncan be part of multiple measures. A measure is constructed using two marks:\nstart and end.\n\nINFO\n\nYou can also access all marks and measures using getMarks and getMeasures\n\n\nDefault measures#\n\nByorg automatically gathers performance data. Middleware measures are collected\nin two separate phases: before handling the response and after it.\n\n","routePath":"/docs/core/performance","lang":"","toc":[{"text":"Measures vs Marks","id":"measures-vs-marks","depth":2,"charIndex":347},{"text":"Default measures","id":"default-measures","depth":2,"charIndex":852}],"domain":"","frontmatter":{},"version":""},{"id":5,"title":"Plugins","content":"#\n\nPlugins allow you to modify the context before it reaches the inference and AI\nresponse phase. Each plugin consists of a name, optional middleware, and\noptional effects.\n\n\nMiddleware and Effects#\n\nWhile middleware and effects share some similarities, they serve different\npurposes within the framework:\n\n\nMiddleware#\n\n * A middleware function receives the request context and an asynchronous next()\n   function, which triggers the next middleware in the chain.\n * Middleware can execute code before and after receiving the final response\n   from the chat model:\n   * Code before the await next() call runs prior to receiving the chat model's\n     response.\n   * Code after the await next() call runs after the chat model's response is\n     received.\n * Middleware execution blocks the final response from being sent to the user.\n   To avoid delaying the user response, use effects instead.\n * Partial responses (enabled via the onPartialResponse option) are streamed\n   immediately as they are received from the chat model. These occur before the\n   middleware code that runs after the await next() call.\n\n\nEffects#\n\n * An effect function received the request context and the final response sent\n   to the user.\n * Effects are executed after the message processing pipeline has completed.\n * Use effects for tasks like logging, analytics, or other post-processing\n   operations that do not block the user response.\n\nNote that the response to the user is blocked until all middlewares finish\nprocessing.\n\n\nMiddleware Example#\n\nLet's create a middleware that enriches the context for our system prompt\nfunction.\n\n\n\n\nEffect example#\n\nNow, let's create an effect that runs after receiving a response from the AI. If\nthe user is an admin or the response ends with an error, it does nothing.\nOtherwise, it increases the message count for the user.\n\n\n\n\nConnecting Plugins#\n\nOnce you've written your plugins, connect them to the app:\n\n\n\nThe order of plugins is important! Depending on the call to next, they are\nexecuted:\n\n * Top-down before the call to next\n * Bottom-up after the call to next\n\n\nMiddleware early return#\n\nYour middleware can also break the execution chain early, stopping the execution\nof any subsequent middleware.\n\n\n\n\nPending effects#\n\nWhen you trigger processMessages on a byorg app, one of the returned values is\npendingEffects. This allows you to wait for them to finish execution, which is\nuseful to prevent the application from shutting down prematurely (e.g., in\nserverless functions).","routePath":"/docs/core/plugins","lang":"","toc":[{"text":"Middleware and Effects","id":"middleware-and-effects","depth":2,"charIndex":174},{"text":"Middleware","id":"middleware","depth":3,"charIndex":307},{"text":"Effects","id":"effects","depth":3,"charIndex":1109},{"text":"Middleware Example","id":"middleware-example","depth":2,"charIndex":1507},{"text":"Effect example","id":"effect-example","depth":2,"charIndex":1616},{"text":"Connecting Plugins","id":"connecting-plugins","depth":2,"charIndex":1848},{"text":"Middleware early return","id":"middleware-early-return","depth":2,"charIndex":2091},{"text":"Pending effects","id":"pending-effects","depth":2,"charIndex":2232}],"domain":"","frontmatter":{},"version":""},{"id":6,"title":"References","content":"#\n\nReferences provide information about the source of data retrieved by a tool\ncall. When a tool call is triggered, you can add references to the context and\nlater use these entries in a plugin to provide users with referenced pages or\nother relevant information. References are part of the context.\n\nINFO\n\nReferences are not automatically added for the AI. You need to implement this\nfunctionality if needed.\n\nLet's create a tool that adds relevant weather information to the context.\n\n\n\nIn this example, the AI receives information about the requested city, and the\ncontext includes information about the source of this data.\n\n\nUsing References#\n\nThe references object provides two functions: getReferences and addReference. If\nyou want to present users with information about references, you need to\nmanually add them to the response.\n\n\n\nBy using these functions, you can ensure that users are informed about the\nsources of the information they receive.","routePath":"/docs/core/references","lang":"","toc":[{"text":"Using References","id":"using-references","depth":2,"charIndex":629}],"domain":"","frontmatter":{},"version":""},{"id":7,"title":"System Prompt","content":"#\n\nThe createApp function requires a systemPrompt function to operate correctly.\nThe system prompt is a string that provides an \"initial\" description of the\nsituation for the AI. It should include details such as the assistant's\npersonality, name, purpose, and available tools. The system prompt can also\nincorporate dynamic values like the current date or time. Think of the system\nprompt as your assistant's \"personality\" and guidelines\n\n\nWhy a function?#\n\nByorg requires a function to generate the system prompt because each message\nmight need a different prompt. This allows you to implement custom logic to\nmodify the system prompt at runtime.\n\n\nExample#\n\nHere is an example that adds the current date and the user's name to the\nconversation:\n\n\n\nAs you can see, the systemPrompt function takes a context parameter. The context\nis an object containing information about the current conversation. We’ll\nexplore this topic further in the next section.","routePath":"/docs/core/system-prompt","lang":"","toc":[{"text":"Why a function?","id":"why-a-function","depth":2,"charIndex":440},{"text":"Example","id":"example","depth":2,"charIndex":650}],"domain":"","frontmatter":{},"version":""},{"id":8,"title":"Tools","content":"#\n\nYou can extend the capabilities of your assistant by providing it with tools.\nTools are functions attached to the AI request. Once the request is received,\nthe AI can decide to use these tools to enrich the context, fetch more data, or\nsave information.\n\nINFO\n\nOpenAI has implemented tools, but not all AI providers do.\n\n\nAdding tools#\n\nTo start, implement the tool function. It takes two inputs: params, which are\ndetermined by the AI, and context, which is passed by byorg. The tool function\nmust return a string, as this information will be passed back to the AI as an\naddition to the system prompt.\n\n\n\nNext, describe this function for the AI to help it understand when and how to\nuse it.\n\n\n\nOnce the function is implemented and described, wrap it into the plugin system\nand connect it to the app.\n\n\n\nTools are an excellent place to implement embedding for inserts or other RAG\n(Retrieval-Augmented Generation) functionalities.\n\nYour tools might attach data from a specific source. If you want to inform users\nabout the data source, you can use references from the context object. We’ll\ndiscuss that next.","routePath":"/docs/core/tools","lang":"","toc":[{"text":"Adding tools","id":"adding-tools","depth":2,"charIndex":324}],"domain":"","frontmatter":{},"version":""},{"id":9,"title":"Usage","content":"#\n\n\nCreating an app#\n\nHere is step by step how to start with byorg.\n\nThat's it! Your message is being handled and processed by byorg.\n\nIn next sections we will go through customisation, error handling and changing\nLLM provider, to better suit your needs.","routePath":"/docs/core/usage","lang":"","toc":[{"text":"Creating an app","id":"creating-an-app","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":""},{"id":10,"title":"Getting Started","content":"#\n\nTo start using byorg.ai, you don't need an extensive setup. All you need is an\nLLM provider of your choice.\n\n\nPre-requisites#\n\n * API keys to LLM Provider supported by Vercel's AI SDK (e.g. OpenAI,\n   Anthropic, ...) complete list here.\n\nTIP\n\nYou can even use a self-hosted LLM, as long as it is compatible with the Vercel\nAI SDK.\n\n\nInstallation#\n\nThe easiest way to begin is by installing byorg.ai packages and ai package using\nyour preferred package manager.\n\nIn the next section, we'll demonstrate how to use our core library to handle\nyour requests.","routePath":"/docs/getting-started","lang":"","toc":[{"text":"Pre-requisites","id":"pre-requisites","depth":2,"charIndex":112},{"text":"Installation","id":"installation","depth":2,"charIndex":335}],"domain":"","frontmatter":{},"version":""},{"id":11,"title":"Integrating with Discord","content":"#\n\nByorg provides built-in functionality to integrate your application with\nDiscord. To set this up, use the createDiscordApp function and provide the\nnecessary parameters.\n\n\nEndpoint mode#\n\n\n\nWARNING\n\nDiscord implementation is still in progress, treat it as experimental.","routePath":"/docs/integrations/discord-usage","lang":"","toc":[{"text":"Endpoint mode","id":"endpoint-mode","depth":2,"charIndex":174}],"domain":"","frontmatter":{},"version":""},{"id":12,"title":"","content":"Integrating with Slack#\n\nByorg provides built-in functionality to integrate your application with Slack.\nTo set this up, use the createSlackApp function and provide the necessary\nparameters.\n\n\nHttp endpoint mode#\n\nIn this mode you use SlackApp to receive Slack event objects directly. Here's a\ntutorial on setting up a Google Cloud Function with event receiver.\n\n\n\nOnce you have an instance of an app wrapped in slack handler, you need to pass\nit the event received from slack endpoint.\n\n\n\nWe provide automatic parsing from slack event to our internal event. We also\nprovide custom formatter to slack blocks that uses slack-rich-text package.\n\n\nWebsocket mode#\n\nAlternatively, you can use Slack SDK ability to connect to Slack API using\nWebSockets. This can be helpful in cases when you want to setup your server in a\nsetting without public IP connection and/or for development purposes.\n\n\n\n\nTypes#\n\nWhen using byorg-slack package, the context.extras field will contain various\nSlack-related fields.\n\n","routePath":"/docs/integrations/slack-usage","lang":"","toc":[{"text":"Integrating with Slack","id":"integrating-with-slack","depth":2,"charIndex":-1},{"text":"Http endpoint mode","id":"http-endpoint-mode","depth":2,"charIndex":192},{"text":"Websocket mode","id":"websocket-mode","depth":2,"charIndex":644},{"text":"Types","id":"types","depth":2,"charIndex":891}],"domain":"","frontmatter":{},"version":""}]